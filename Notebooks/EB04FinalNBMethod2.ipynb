{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Capstone EB04**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following version of the code is for finding like minded user communities by getting higher level topics during the lda phase before performing clustering. This is the second of two different methods that were attempted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ragulan550/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ragulan550/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import models\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import json\n",
    "import warnings\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.corpus import words\n",
    "eng_words = words.words('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed User Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>ConceptText</th>\n",
       "      <th>userid</th>\n",
       "      <th>creationtimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11123785801404416</td>\n",
       "      <td>tweet RT</td>\n",
       "      <td>Twitter RT (TV network)</td>\n",
       "      <td>142685766</td>\n",
       "      <td>2010-12-04 18:24:51 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16801364319404032</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>White American</td>\n",
       "      <td>81450435</td>\n",
       "      <td>2010-12-20 10:25:32 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8977557407928321</td>\n",
       "      <td>smiley face</td>\n",
       "      <td>Smiley</td>\n",
       "      <td>89099440</td>\n",
       "      <td>2010-11-28 20:16:31 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10017331400941568</td>\n",
       "      <td>kami sama rin</td>\n",
       "      <td>Kami Japanese honorifics Japanese yen</td>\n",
       "      <td>142962699</td>\n",
       "      <td>2010-12-01 17:08:12 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18759561343143936</td>\n",
       "      <td>king William the Conqueror England crowned</td>\n",
       "      <td>Charles I of England William the Conqueror Kin...</td>\n",
       "      <td>22619937</td>\n",
       "      <td>2010-12-25 20:06:42 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                   TweetText  \\\n",
       "0  11123785801404416                                    tweet RT   \n",
       "1  16801364319404032                                       WHITE   \n",
       "2   8977557407928321                                 smiley face   \n",
       "3  10017331400941568                               kami sama rin   \n",
       "4  18759561343143936  king William the Conqueror England crowned   \n",
       "\n",
       "                                         ConceptText     userid  \\\n",
       "0                            Twitter RT (TV network)  142685766   \n",
       "1                                     White American   81450435   \n",
       "2                                             Smiley   89099440   \n",
       "3              Kami Japanese honorifics Japanese yen  142962699   \n",
       "4  Charles I of England William the Conqueror Kin...   22619937   \n",
       "\n",
       "         creationtimestamp  \n",
       "0  2010-12-04 18:24:51 UTC  \n",
       "1  2010-12-20 10:25:32 UTC  \n",
       "2  2010-11-28 20:16:31 UTC  \n",
       "3  2010-12-01 17:08:12 UTC  \n",
       "4  2010-12-25 20:06:42 UTC  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loads csv from stored location\n",
    "df = pd.read_csv('../csvfiles/tweetsOnUserOnConcepts.csv', lineterminator='\\n', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all concept text for each tweet and stores in list\n",
    "tweetConcept = df.ConceptText.values.tolist()\n",
    "userIds = df.userid.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores a list of each tweet and the words the tweets contain\n",
    "tempData = []\n",
    "\n",
    "for sent in tweetConcept:\n",
    "    x = []\n",
    "    for word in sent.split(\" \"):\n",
    "        x.append(word)\n",
    "    tempData.append(x)\n",
    "\n",
    "data_final = tempData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up corpus for lda\n",
    "id2word1 = corpora.Dictionary(data_final)\n",
    "texts = data_final\n",
    "corpus1 = [id2word1.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA\n",
    "Multicore allows for multiple cores to be working on LDA simultaneously\n",
    "- Check Number of workers\n",
    "- Check Number of topics set<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35,\n",
      "  '0.139*\"Thanksgiving\" + 0.042*\"Turkey\" + 0.030*\"Unemployment\" + 0.021*\"Star\" '\n",
      "  '+ 0.021*\"Mark\" + 0.021*\"market\" + 0.021*\"(bird)\" + 0.020*\"Foreign\" + '\n",
      "  '0.018*\"Pour\" + 0.018*\"Jalili\"'),\n",
      " (1,\n",
      "  '0.241*\"New\" + 0.096*\"York\" + 0.060*\"Day\" + 0.059*\"International\" + '\n",
      "  '0.055*\"Year\\'s\" + 0.048*\"Airport\" + 0.046*\"City\" + 0.024*\"Eve\" + '\n",
      "  '0.023*\"Jersey\" + 0.018*\"of\"'),\n",
      " (33,\n",
      "  '0.050*\"San\" + 0.034*\"Francisco\" + 0.030*\"Climate\" + 0.024*\"with\" + '\n",
      "  '0.024*\"For\" + 0.023*\"Australia\" + 0.022*\"Family\" + 0.021*\"Snow\" + '\n",
      "  '0.019*\"Economist\" + 0.019*\"Transmitter\"'),\n",
      " (29,\n",
      "  '0.123*\"California\" + 0.055*\"Service\" + 0.037*\"Africa\" + 0.034*\"ITunes\" + '\n",
      "  '0.026*\"Atlanta\" + 0.025*\"Group\" + 0.025*\"Americans\" + 0.022*\"Georgia\" + '\n",
      "  '0.019*\"television\" + 0.018*\"Now\"'),\n",
      " (23,\n",
      "  '0.187*\"the\" + 0.127*\"of\" + 0.107*\"United\" + 0.082*\"States\" + 0.063*\"in\" + '\n",
      "  '0.031*\"Israel\" + 0.028*\"Kingdom\" + 0.023*\"President\" + 0.015*\"Press\" + '\n",
      "  '0.011*\"and\"'),\n",
      " (34,\n",
      "  '0.121*\"(film)\" + 0.051*\"Palin\" + 0.050*\"Sarah\" + 0.025*\"Brazil\" + '\n",
      "  '0.023*\"Alaska\" + 0.023*\"Green\" + 0.023*\"service\" + 0.023*\"Man\" + '\n",
      "  '0.018*\"Wind\" + 0.016*\"Lee\"'),\n",
      " (25,\n",
      "  '0.062*\"football)\" + 0.059*\"(American\" + 0.050*\"CNN\" + 0.049*\"Federal\" + '\n",
      "  '0.043*\"(magazine)\" + 0.036*\"Tackle\" + 0.026*\"Commission\" + '\n",
      "  '0.023*\"Communications\" + 0.022*\"Spanish\" + 0.018*\"System\"'),\n",
      " (27,\n",
      "  '0.172*\"United\" + 0.144*\"States\" + 0.130*\"WikiLeaks\" + 0.062*\"RT\" + '\n",
      "  '0.062*\"network)\" + 0.060*\"(TV\" + 0.033*\"Senate\" + 0.032*\"Julian\" + '\n",
      "  '0.031*\"Assange\" + 0.029*\"leak\"'),\n",
      " (45,\n",
      "  '0.106*\"of\" + 0.046*\"List\" + 0.044*\"Security\" + 0.041*\"La\" + '\n",
      "  '0.032*\"Department\" + 0.025*\"Court\" + 0.025*\"Latin-script\" + '\n",
      "  '0.025*\"digraphs\" + 0.022*\"King\" + 0.021*\"Administration\"'),\n",
      " (20,\n",
      "  '0.070*\"Time\" + 0.066*\"State\" + 0.061*\"Mexico\" + 0.058*\"film)\" + '\n",
      "  '0.039*\"Central\" + 0.034*\"Agency\" + 0.024*\"of\" + 0.024*\"Home\" + 0.021*\"NATO\" '\n",
      "  '+ 0.020*\"(2012\"'),\n",
      " (28,\n",
      "  '0.119*\"Obama\" + 0.113*\"Barack\" + 0.045*\"of\" + 0.039*\"Japan\" + '\n",
      "  '0.032*\"France\" + 0.021*\"French\" + 0.020*\"Video\" + 0.019*\"Boston\" + '\n",
      "  '0.018*\"Science\" + 0.018*\"Photography\"'),\n",
      " (39,\n",
      "  '0.045*\"music\" + 0.034*\"Email\" + 0.033*\"(computing)\" + 0.032*\"Venezuela\" + '\n",
      "  '0.025*\"I\\'m\" + 0.020*\"Mashable\" + 0.019*\"Suicide\" + 0.018*\"Stock\" + '\n",
      "  '0.016*\"Rome\" + 0.013*\"hop\"'),\n",
      " (44,\n",
      "  '0.065*\"YouTube\" + 0.056*\"Show\" + 0.047*\"Europe\" + 0.039*\"for\" + '\n",
      "  '0.034*\"Cancer\" + 0.030*\"Computer\" + 0.021*\"All\" + 0.020*\"Early\" + '\n",
      "  '0.019*\"Song\" + 0.017*\"Orleans\"'),\n",
      " (30,\n",
      "  '0.075*\"ATP\" + 0.062*\"&amp;\" + 0.031*\"Chris\" + 0.030*\"Muslim\" + '\n",
      "  '0.023*\"Islam\" + 0.022*\"Disney\" + 0.020*\"IPod\" + 0.019*\"Walt\" + '\n",
      "  '0.018*\"Johnson\" + 0.018*\"Lawsuit\"'),\n",
      " (17,\n",
      "  '0.101*\"Afghanistan\" + 0.086*\"War\" + 0.030*\"in\" + 0.028*\"Law\" + 0.021*\"of\" + '\n",
      "  '0.019*\"Society\" + 0.018*\"High\" + 0.017*\"Switzerland\" + '\n",
      "  '0.016*\"(2001–present)\" + 0.016*\"Photograph\"'),\n",
      " (7,\n",
      "  '0.158*\"China\" + 0.136*\"Facebook\" + 0.066*\"Pakistan\" + 0.033*\"Daily\" + '\n",
      "  '0.026*\"officer\" + 0.021*\"Chief\" + 0.019*\"News\" + 0.019*\"Energy\" + '\n",
      "  '0.017*\"Information\" + 0.016*\"Solar\"'),\n",
      " (11,\n",
      "  '0.077*\"Police\" + 0.032*\"Your\" + 0.026*\"Red\" + 0.025*\"Winter\" + '\n",
      "  '0.023*\"County,\" + 0.021*\"Foundation\" + 0.021*\"Chinese\" + 0.016*\"(music)\" + '\n",
      "  '0.015*\"Welfare\" + 0.014*\"–\"'),\n",
      " (31,\n",
      "  '0.086*\"(band)\" + 0.072*\"to\" + 0.053*\"In\" + 0.048*\"Tax\" + 0.044*\"(album)\" + '\n",
      "  '0.043*\"Me\" + 0.040*\"de\" + 0.019*\"cut\" + 0.018*\"Club\" + 0.016*\"Paulo\"'),\n",
      " (36,\n",
      "  '0.054*\"Health\" + 0.049*\"Los\" + 0.044*\"Angeles\" + 0.034*\"Terrorism\" + '\n",
      "  '0.034*\"Food\" + 0.024*\"Drug\" + 0.021*\"Zealand\" + 0.019*\"care\" + '\n",
      "  '0.018*\"Cannabis\" + 0.018*\"and\"'),\n",
      " (10,\n",
      "  '0.583*\"Twitter\" + 0.024*\"software\" + 0.019*\"Application\" + 0.014*\"Steve\" + '\n",
      "  '0.011*\"and\" + 0.011*\"Patient\" + 0.010*\"Care\" + 0.009*\"Protection\" + '\n",
      "  '0.009*\"Act\" + 0.009*\"Affordable\"')]\n"
     ]
    }
   ],
   "source": [
    "#uncomment line below to try lda with different values\n",
    "topicNum = 47\n",
    "#lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus1,id2word=id2word1,num_topics=topicNum, passes=10, workers=7)\n",
    "#lda_model.save('../LdaSaves/topics47mar10p9/lda.model_mar10_t47')\n",
    "\n",
    "#preloading an saved lda run to save time as lda takes long time to run\n",
    "lda_model1 =  models.LdaModel.load('../LDASaves/ldamar25/lda.model_mar25_t47')\n",
    "pprint(lda_model1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dictionary where each key is the user and the value is a list of all topicsNums representing each tweet\n",
    "dictConcept = {}\n",
    "\n",
    "for i in range(len(userIds)):\n",
    "    if userIds[i] not in dictConcept:\n",
    "        dictConcept[userIds[i]] = []\n",
    "\n",
    "#getting the top 3 topics for each user tweet and appending to the user dictionary\n",
    "for i, row in enumerate (lda_model1[corpus1]):\n",
    "    sortedValue = sorted(row, key=lambda x:x[1], reverse=True)\n",
    "    userid =int(df.iloc[[i]].userid)\n",
    "    for z in sortedValue[:3]:\n",
    "        dictConcept[userid].append(str(z[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running 2nd Round of LDA To find Higher Level Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8', '23', '41'], ['33', '0', '1']]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "#using previously saved list to save time\n",
    "# topicsPerTweets=[]\n",
    "# with open('topicsPerTweets.txt', 'r') as f:\n",
    "#     for line in f:\n",
    "#         topicsPerTweets.append(ast.literal_eval(line))\n",
    "        \n",
    "topicsPerTweets = list(dictConcept.values())\n",
    "print(topicsPerTweets[:2])\n",
    "\n",
    "id2word2 = corpora.Dictionary(topicsPerTweets)\n",
    "corpus2 = [id2word2.doc2bow(text) for text in topicsPerTweets]\n",
    "\n",
    "#saving the list so the above block does not need to be rerun since it takes a while\n",
    "# with open('topicsPerTweets.txt', 'w') as f:\n",
    "#     for item in topicsPerTweets:\n",
    "#         f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.131*\"7\" + 0.111*\"38\" + 0.093*\"14\" + 0.074*\"16\" + 0.054*\"39\" + 0.050*\"24\" '\n",
      "  '+ 0.040*\"1\" + 0.036*\"26\" + 0.031*\"0\" + 0.025*\"5\"'),\n",
      " (1,\n",
      "  '0.093*\"6\" + 0.086*\"33\" + 0.085*\"9\" + 0.076*\"36\" + 0.069*\"12\" + 0.060*\"44\" + '\n",
      "  '0.052*\"35\" + 0.049*\"1\" + 0.042*\"0\" + 0.031*\"3\"'),\n",
      " (2,\n",
      "  '0.128*\"8\" + 0.105*\"0\" + 0.105*\"4\" + 0.097*\"26\" + 0.088*\"1\" + 0.050*\"37\" + '\n",
      "  '0.039*\"12\" + 0.030*\"31\" + 0.021*\"38\" + 0.020*\"23\"'),\n",
      " (3,\n",
      "  '0.195*\"15\" + 0.169*\"25\" + 0.085*\"29\" + 0.039*\"22\" + 0.038*\"1\" + 0.033*\"3\" + '\n",
      "  '0.025*\"30\" + 0.024*\"2\" + 0.024*\"32\" + 0.022*\"0\"'),\n",
      " (4,\n",
      "  '0.329*\"21\" + 0.189*\"18\" + 0.049*\"1\" + 0.045*\"0\" + 0.033*\"44\" + 0.031*\"37\" + '\n",
      "  '0.031*\"12\" + 0.019*\"40\" + 0.018*\"6\" + 0.018*\"23\"'),\n",
      " (5,\n",
      "  '0.579*\"10\" + 0.066*\"30\" + 0.057*\"4\" + 0.052*\"26\" + 0.052*\"3\" + 0.050*\"21\" + '\n",
      "  '0.038*\"0\" + 0.033*\"1\" + 0.015*\"25\" + 0.014*\"46\"'),\n",
      " (6,\n",
      "  '0.187*\"27\" + 0.069*\"5\" + 0.061*\"32\" + 0.056*\"23\" + 0.046*\"1\" + 0.046*\"6\" + '\n",
      "  '0.044*\"20\" + 0.041*\"17\" + 0.032*\"0\" + 0.030*\"7\"'),\n",
      " (7,\n",
      "  '0.322*\"1\" + 0.314*\"0\" + 0.124*\"2\" + 0.017*\"31\" + 0.012*\"39\" + 0.012*\"45\" + '\n",
      "  '0.011*\"35\" + 0.011*\"14\" + 0.010*\"28\" + 0.009*\"5\"'),\n",
      " (8,\n",
      "  '0.096*\"41\" + 0.091*\"40\" + 0.086*\"11\" + 0.083*\"1\" + 0.077*\"24\" + 0.071*\"31\" '\n",
      "  '+ 0.068*\"19\" + 0.055*\"0\" + 0.032*\"8\" + 0.028*\"33\"'),\n",
      " (9,\n",
      "  '0.119*\"46\" + 0.114*\"23\" + 0.103*\"12\" + 0.069*\"28\" + 0.066*\"42\" + 0.059*\"43\" '\n",
      "  '+ 0.045*\"34\" + 0.033*\"32\" + 0.024*\"13\" + 0.021*\"35\"')]\n"
     ]
    }
   ],
   "source": [
    "topicNum2 = 10\n",
    "today = datetime.datetime.now()\n",
    "#uncomment the line below to run with own custom topics numbers or workers\n",
    "lda_model2 = gensim.models.ldamulticore.LdaMulticore(corpus=corpus2, id2word=id2word2, num_topics=topicNum2, passes=10, workers=3)\n",
    "lda_model2.save('../LDASaves/HigherOrderModels/LDA' + today.strftime(\"%M%d\") + str(topicNum2))\n",
    "\n",
    "lda_model2 =  models.LdaModel.load('../LDASaves/HigherOrderModels/LDA' + today.strftime(\"%M%d\") + str(topicNum2))\n",
    "doc_lda2 = lda_model2[corpus2]\n",
    "pprint(lda_model2.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity and Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -3.5847416341948763\n",
      "\n",
      "Coherence Score:  0.27723398824434303\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model2.log_perplexity(corpus2))  # a measure of how good the model is. lower the better.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model2, texts=topicsPerTweets, dictionary=id2word2, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5658</td>\n",
       "      <td>41, 40, 11, 1, 24, 31, 19, 0, 8, 33</td>\n",
       "      <td>[8, 23, 41]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5231</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>[33, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2938</td>\n",
       "      <td>7, 38, 14, 16, 39, 24, 1, 26, 0, 5</td>\n",
       "      <td>[29, 16, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>6, 33, 9, 36, 12, 44, 35, 1, 0, 3</td>\n",
       "      <td>[12, 13, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.4757</td>\n",
       "      <td>46, 23, 12, 28, 42, 43, 34, 32, 13, 35</td>\n",
       "      <td>[23, 30, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>[40, 33, 33, 27, 27, 17, 8, 3, 30, 0, 1, 28, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.3862</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>[10, 18, 2, 28, 30, 25, 0, 1, 0, 1, 2, 15, 34,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.4257</td>\n",
       "      <td>15, 25, 29, 22, 1, 3, 30, 2, 32, 0</td>\n",
       "      <td>[21, 10, 6, 22, 29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.8306</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>[40, 0, 1, 35, 0, 1, 16, 0, 1, 31, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>[31, 30, 4, 33, 25, 10, 11, 26, 16, 0, 1, 2, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             8.0              0.5658   \n",
       "1            1             7.0              0.5231   \n",
       "2            2             0.0              0.2938   \n",
       "3            3             1.0              0.7750   \n",
       "4            4             9.0              0.4757   \n",
       "5            5             7.0              0.3070   \n",
       "6            6             7.0              0.3862   \n",
       "7            7             3.0              0.4257   \n",
       "8            8             7.0              0.8306   \n",
       "9            9             7.0              0.2713   \n",
       "\n",
       "                                 Keywords  \\\n",
       "0     41, 40, 11, 1, 24, 31, 19, 0, 8, 33   \n",
       "1      1, 0, 2, 31, 39, 45, 35, 14, 28, 5   \n",
       "2      7, 38, 14, 16, 39, 24, 1, 26, 0, 5   \n",
       "3       6, 33, 9, 36, 12, 44, 35, 1, 0, 3   \n",
       "4  46, 23, 12, 28, 42, 43, 34, 32, 13, 35   \n",
       "5      1, 0, 2, 31, 39, 45, 35, 14, 28, 5   \n",
       "6      1, 0, 2, 31, 39, 45, 35, 14, 28, 5   \n",
       "7      15, 25, 29, 22, 1, 3, 30, 2, 32, 0   \n",
       "8      1, 0, 2, 31, 39, 45, 35, 14, 28, 5   \n",
       "9      1, 0, 2, 31, 39, 45, 35, 14, 28, 5   \n",
       "\n",
       "                                                Text  \n",
       "0                                        [8, 23, 41]  \n",
       "1                                         [33, 0, 1]  \n",
       "2                                       [29, 16, 10]  \n",
       "3                                        [12, 13, 3]  \n",
       "4                                       [23, 30, 38]  \n",
       "5  [40, 33, 33, 27, 27, 17, 8, 3, 30, 0, 1, 28, 1...  \n",
       "6  [10, 18, 2, 28, 30, 25, 0, 1, 0, 1, 2, 15, 34,...  \n",
       "7                                [21, 10, 6, 22, 29]  \n",
       "8           [40, 0, 1, 35, 0, 1, 16, 0, 1, 31, 0, 1]  \n",
       "9  [31, 30, 4, 33, 25, 10, 11, 26, 16, 0, 1, 2, 2...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model2, corpus=corpus2, texts=topicsPerTweets):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model2, corpus=corpus2, texts=topicsPerTweets)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>41, 40, 11, 1, 24, 31, 19, 0, 8, 33</td>\n",
       "      <td>9557.0</td>\n",
       "      <td>0.0852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>11226.0</td>\n",
       "      <td>0.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7, 38, 14, 16, 39, 24, 1, 26, 0, 5</td>\n",
       "      <td>10876.0</td>\n",
       "      <td>0.0969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6, 33, 9, 36, 12, 44, 35, 1, 0, 3</td>\n",
       "      <td>7730.0</td>\n",
       "      <td>0.0689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>46, 23, 12, 28, 42, 43, 34, 32, 13, 35</td>\n",
       "      <td>3877.0</td>\n",
       "      <td>0.0345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>0.0175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>9386.0</td>\n",
       "      <td>0.0836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>15, 25, 29, 22, 1, 3, 30, 2, 32, 0</td>\n",
       "      <td>33482.0</td>\n",
       "      <td>0.2984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>11827.0</td>\n",
       "      <td>0.1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1, 0, 2, 31, 39, 45, 35, 14, 28, 5</td>\n",
       "      <td>12297.0</td>\n",
       "      <td>0.1096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dominant_Topic                          Topic_Keywords  Num_Documents  \\\n",
       "0             8.0     41, 40, 11, 1, 24, 31, 19, 0, 8, 33         9557.0   \n",
       "1             7.0      1, 0, 2, 31, 39, 45, 35, 14, 28, 5        11226.0   \n",
       "2             0.0      7, 38, 14, 16, 39, 24, 1, 26, 0, 5        10876.0   \n",
       "3             1.0       6, 33, 9, 36, 12, 44, 35, 1, 0, 3         7730.0   \n",
       "4             9.0  46, 23, 12, 28, 42, 43, 34, 32, 13, 35         3877.0   \n",
       "5             7.0      1, 0, 2, 31, 39, 45, 35, 14, 28, 5         1960.0   \n",
       "6             7.0      1, 0, 2, 31, 39, 45, 35, 14, 28, 5         9386.0   \n",
       "7             3.0      15, 25, 29, 22, 1, 3, 30, 2, 32, 0        33482.0   \n",
       "8             7.0      1, 0, 2, 31, 39, 45, 35, 14, 28, 5        11827.0   \n",
       "9             7.0      1, 0, 2, 31, 39, 45, 35, 14, 28, 5        12297.0   \n",
       "\n",
       "   Perc_Documents  \n",
       "0          0.0852  \n",
       "1          0.1000  \n",
       "2          0.0969  \n",
       "3          0.0689  \n",
       "4          0.0345  \n",
       "5          0.0175  \n",
       "6          0.0836  \n",
       "7          0.2984  \n",
       "8          0.1054  \n",
       "9          0.1096  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating User Vectors of length K where K is number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.2756533)\n",
      "Shows a sample userVector\n",
      "[0.025003769, 0.27573124, 0.025003202, 0.025001932, 0.025001537, 0.025001056, 0.02500354, 0.52424777, 0.025005233, 0.025000773]\n"
     ]
    }
   ],
   "source": [
    "UserVectors = []\n",
    "\n",
    "#for each users shows percent contribution for that topic\n",
    "print(lda_model2[corpus2][1][1])\n",
    "\n",
    "for row in lda_model2[corpus2]:\n",
    "    temp = [0]*topicNum2\n",
    "    for val in row:\n",
    "        #val is a tuple in form (topicNum, percentContributionOfTopicToUser)\n",
    "        temp[val[0]] = val[1]\n",
    "    UserVectors.append(temp)\n",
    "    \n",
    "print(\"Shows a sample userVector\")    \n",
    "print(UserVectors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Gold Standard News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>creationtimestamp</th>\n",
       "      <th>NewsId</th>\n",
       "      <th>NewsText</th>\n",
       "      <th>NewsConceptText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://on.cnn.com/9BMsbh</td>\n",
       "      <td>5813001621872640</td>\n",
       "      <td>32814009</td>\n",
       "      <td>2010-11-20 02:41:42 UTC</td>\n",
       "      <td>50637</td>\n",
       "      <td>pharmaceutical companies Big Pharma OH MY GAWD...</td>\n",
       "      <td>Pharmaceutical industry Pharmaceutical industr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://money.cnn.com/2010/12/23/pf/rich_wealth...</td>\n",
       "      <td>18123124054695937</td>\n",
       "      <td>18097177</td>\n",
       "      <td>2010-12-24 01:57:44 UTC</td>\n",
       "      <td>76310</td>\n",
       "      <td>net worth mortgages economist survey of consum...</td>\n",
       "      <td>Wealth Mortgage loan Economist Survey of Consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.cnn.com/2010/SHOWBIZ/celebrity.news...</td>\n",
       "      <td>3517403661074433</td>\n",
       "      <td>68520890</td>\n",
       "      <td>2010-11-13 18:39:49 UTC</td>\n",
       "      <td>48949</td>\n",
       "      <td>wheelchair Toulouse-Lautrec Los Angeles, Calif...</td>\n",
       "      <td>Wheelchair Henri de Toulouse-Lautrec Los Angel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://y.ahoo.it/FzXGKS</td>\n",
       "      <td>6629109681623040</td>\n",
       "      <td>113850982</td>\n",
       "      <td>2010-11-22 08:44:37 UTC</td>\n",
       "      <td>52814</td>\n",
       "      <td>friends. You Riyadh Google Groups amd no free ...</td>\n",
       "      <td>FriendsWithYou Riyadh Google Groups Advanced M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://on.cnn.com/ic2iYo</td>\n",
       "      <td>6940757323677696</td>\n",
       "      <td>759251</td>\n",
       "      <td>2010-11-23 05:23:00 UTC</td>\n",
       "      <td>50693</td>\n",
       "      <td>thing. I Mitt Romney Massachusetts George Bush...</td>\n",
       "      <td>Treehouse of Horror VII Mitt Romney Massachuse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url            tweetid  \\\n",
       "0                           http://on.cnn.com/9BMsbh   5813001621872640   \n",
       "1  http://money.cnn.com/2010/12/23/pf/rich_wealth...  18123124054695937   \n",
       "2  http://www.cnn.com/2010/SHOWBIZ/celebrity.news...   3517403661074433   \n",
       "3                            http://y.ahoo.it/FzXGKS   6629109681623040   \n",
       "4                           http://on.cnn.com/ic2iYo   6940757323677696   \n",
       "\n",
       "      userid        creationtimestamp  NewsId  \\\n",
       "0   32814009  2010-11-20 02:41:42 UTC   50637   \n",
       "1   18097177  2010-12-24 01:57:44 UTC   76310   \n",
       "2   68520890  2010-11-13 18:39:49 UTC   48949   \n",
       "3  113850982  2010-11-22 08:44:37 UTC   52814   \n",
       "4     759251  2010-11-23 05:23:00 UTC   50693   \n",
       "\n",
       "                                            NewsText  \\\n",
       "0  pharmaceutical companies Big Pharma OH MY GAWD...   \n",
       "1  net worth mortgages economist survey of consum...   \n",
       "2  wheelchair Toulouse-Lautrec Los Angeles, Calif...   \n",
       "3  friends. You Riyadh Google Groups amd no free ...   \n",
       "4  thing. I Mitt Romney Massachusetts George Bush...   \n",
       "\n",
       "                                     NewsConceptText  \n",
       "0  Pharmaceutical industry Pharmaceutical industr...  \n",
       "1  Wealth Mortgage loan Economist Survey of Consu...  \n",
       "2  Wheelchair Henri de Toulouse-Lautrec Los Angel...  \n",
       "3  FriendsWithYou Riyadh Google Groups Advanced M...  \n",
       "4  Treehouse of Horror VII Mitt Romney Massachuse...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfGoldStandard = pd.read_csv('../csvfiles/GoldStandard.csv',  lineterminator='\\n', low_memory=False)\n",
    "dfGoldStandard.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsUserId = dfGoldStandard.userid.values.tolist()\n",
    "newsUrl = dfGoldStandard.url.values.tolist()\n",
    "newsId = dfGoldStandard.NewsId.values.tolist()\n",
    "\n",
    "#dictionary of users who posted a newsArticle\n",
    "newsId2UserId = {}\n",
    "\n",
    "for i in range(len(newsId)):\n",
    "    if newsId[i] not in newsId2UserId:\n",
    "        newsId2UserId[newsId[i]] = []\n",
    "    newsId2UserId[newsId[i]].append(newsUserId[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading another dataframe with goldstandard but only keeping unique newsids\n",
    "dfUniqueNewsId = pd.read_csv('../csvfiles/GoldStandard.csv',  lineterminator='\\n', low_memory=False)\n",
    "dfUniqueNewsId.drop_duplicates(subset='NewsId', inplace = True)\n",
    "newsArticles = dfUniqueNewsId.NewsConceptText.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting 1st Round LDA topics for news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing words in news articles in a list\n",
    "newsArticlesForCorpus = [x.split(' ') for x in newsArticles]\n",
    "#creating a corpus\n",
    "newsId2word = corpora.Dictionary(newsArticlesForCorpus)\n",
    "NewsArticlesCorpus = [newsId2word.doc2bow(text) for text in newsArticlesForCorpus]\n",
    "\n",
    "#using the previous lda_model for first lda run with the news corpus created to get a percent contribution for each topic for each news article\n",
    "TopicDistributionOnNewsArticles = lda_model1[NewsArticlesCorpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['34', '8', '10'], ['23', '22', '46'], ['4', '8', '15'], ['0', '24', '8'], ['32', '44', '25']]\n"
     ]
    }
   ],
   "source": [
    "# finding the top 3 topics for each news article\n",
    "topicsPerNewsArticleHighLevel = []\n",
    "for x in (TopicDistributionOnNewsArticles):\n",
    "    sortedValue = sorted(x, key=lambda x:x[1], reverse=True)\n",
    "    temp = []\n",
    "    for z in sortedValue[:3]:\n",
    "        temp.append(str(z[0]))\n",
    "    topicsPerNewsArticleHighLevel.append(temp)\n",
    "\n",
    "print(topicsPerNewsArticleHighLevel[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting 2nd Round LDA topics (High Level Topics) for news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up corpus for 2nd lda run on news articles\n",
    "newsId2word2 = corpora.Dictionary(topicsPerNewsArticleHighLevel)\n",
    "NewsArticlesCorpus2 = [newsId2word2.doc2bow(text) for text in topicsPerNewsArticleHighLevel]\n",
    "TopicDistributionOnNewsArticles = lda_model2[NewsArticlesCorpus2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating User Vectors of length K where K is number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying sample article vector\n",
      "[0.025003776, 0.27553427, 0.025003204, 0.025001936, 0.025001539, 0.025001058, 0.025003547, 0.5244446, 0.025005287, 0.025000777]\n"
     ]
    }
   ],
   "source": [
    "ArticleVector = []\n",
    "\n",
    "for row in TopicDistributionOnNewsArticles:\n",
    "    temp = [0]*topicNum2\n",
    "    for val in row:\n",
    "        #val is a tuple in form (topicNum, percentContributionOfTopicToUser)\n",
    "        temp[val[0]] = val[1]\n",
    "    ArticleVector.append(temp)\n",
    "    \n",
    "print(\"Displaying sample article vector\")\n",
    "print(ArticleVector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and preloading Kmeans results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different cluster sizes to try out analysis for\n",
    "numClusters=[5, 10, 15, 20, 25, 30]\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "#saving kmeans results for the differnt cluster sizes\n",
    "for x in range(len(numClusters)):\n",
    "    userVectorsFit = np.array(UserVectors)\n",
    "    #performing kmeans on the userVector to cluster users into communities\n",
    "    kmeans = KMeans(n_clusters=numClusters[x], random_state=0).fit(userVectorsFit)\n",
    "    \n",
    "    kMeansfilename = 'LDAM2-kMeans'+ today.strftime(\"%M%d\") + 'CSize' + str(numClusters[x])\n",
    "    pickle.dump(kmeans, open(\"../kmeansFiles/\" + kMeansfilename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen File: 'LDAM2-kMeans1404CSize30'\n"
     ]
    }
   ],
   "source": [
    "#change this number to a number from the [5, 10, 15, 20, 25, 30] to preload a different file\n",
    "chosenNumberOfCluster = 30\n",
    "\n",
    "#loading existing kmeans model\n",
    "kMeansfilename = 'LDAM2-kMeans' + today.strftime(\"%M%d\") + 'CSize' + str(chosenNumberOfCluster)\n",
    "print('Chosen File: \\''+kMeansfilename+'\\'')\n",
    "\n",
    "loadedKmeansModel = pickle.load(open(\"../kmeansFiles/\" + kMeansfilename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users in each Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12453, 4082, 1667, 6022, 1458, 4969, 1229, 2526, 3356, 3832, 2846, 4338, 3842, 4305, 3366, 2048, 4078, 2253, 5937, 4338, 3840, 3749, 3780, 4793, 2469, 3621, 1345, 4223, 1506, 3947]\n"
     ]
    }
   ],
   "source": [
    "#creating a list to show how many users are in each cluster\n",
    "userClusters = [0]*chosenNumberOfCluster\n",
    "for i in loadedKmeansModel.labels_:\n",
    "    userClusters[i] += 1\n",
    "\n",
    "print(userClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Indexes in each cluster, organized as an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserIndexInCluster=[]\n",
    "idsDict = list(dictConcept.keys())\n",
    "\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    UserIndexInCluster.append([])\n",
    "    \n",
    "for index, val in enumerate(loadedKmeansModel.labels_):\n",
    "    UserIndexInCluster[val].append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User ***IDs*** in each cluster, organized as an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idsCluster = []\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    idsCluster.append([])\n",
    "    \n",
    "for index, val in enumerate(loadedKmeansModel.labels_):\n",
    "    idsCluster[val].append(idsDict[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Topic Distribution Per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicDistributionPerCluster=[]\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    topicDistributionPerCluster.append([])\n",
    "    \n",
    "for i,cluster in enumerate(UserIndexInCluster):\n",
    "    for userIndex in cluster:\n",
    "        topicDistributionPerCluster[i].append(UserVectors[userIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Average Topic distribution per Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03202455 0.03888813 0.32654442 0.03580891 0.02753587 0.02751216\n",
      " 0.02991044 0.39240489 0.04464912 0.03473951]\n"
     ]
    }
   ],
   "source": [
    "averageDistributionPerCluster = []\n",
    "for x in topicDistributionPerCluster:\n",
    "    y = np.array(x)\n",
    "    listOfAverageValues = np.mean(y,axis=0)\n",
    "    averageDistributionPerCluster.append(listOfAverageValues)\n",
    "print(listOfAverageValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Articles to a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "rankArticlesToCluster=[]\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    rankArticlesToCluster.append([])\n",
    "    \n",
    "for x in range (len(ArticleVector)):\n",
    "    for index,value in enumerate(averageDistributionPerCluster):\n",
    "        #finds cosine similarity between artlice vector and average vector of the cluster\n",
    "        rankArticlesToCluster[index].append(tuple((x,1 - spatial.distance.cosine(ArticleVector[x], value))))\n",
    "        \n",
    "#sorting the ranked list\n",
    "import operator\n",
    "sortedRankArticlesToCluster=[]\n",
    "for x in rankArticlesToCluster:\n",
    "    sortedRankArticlesToCluster.append(sorted(x,key=lambda x: x[1]))\n",
    "\n",
    "ascendingRankedArticlesToCluster = []\n",
    "for x in sortedRankArticlesToCluster:\n",
    "    ascendingRankedArticlesToCluster.append(list(reversed(x)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Clusters to an Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankClustersToArticle = []\n",
    "for x in range(len(ArticleVector)):\n",
    "    rankClustersToArticle.append([])\n",
    "    \n",
    "for x in range(chosenNumberOfCluster):\n",
    "    for index, value in enumerate(ArticleVector):\n",
    "        rankClustersToArticle[index].append(tuple((x, 1-spatial.distance.cosine(value, averageDistributionPerCluster[x]))))\n",
    "        \n",
    "#sorting the ranked list\n",
    "\n",
    "sortedRankClustersToArticle=[]\n",
    "for x in rankClustersToArticle:\n",
    "    sortedRankClustersToArticle.append(sorted(x,key=lambda x: x[1]))\n",
    "\n",
    "ascendingRankClustersToArticle = []\n",
    "for x in sortedRankClustersToArticle:\n",
    "    ascendingRankClustersToArticle.append(list(reversed(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S@10 Version 1 where we compare if one user who posted the aricle exists in the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sAt10OneUser():\n",
    "    k=10\n",
    "    total=0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        count = 0;\n",
    "        for y in ascendingRankedArticlesToCluster[x][:k]:\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            for user in newsId2UserId[newsid]:\n",
    "                if user in idsCluster[x]:\n",
    "                    count+=10\n",
    "                    total+=10\n",
    "                    break\n",
    "            if count != 0:\n",
    "                break\n",
    "    precisionVal = total/(chosenNumberOfCluster*10)\n",
    "    print(precisionVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S@10 Version 2 where we compare if all users who posted the aricle exists in the community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sAt10AllUsers():\n",
    "    k=10\n",
    "    total=0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        count = 0;\n",
    "        for y in ascendingRankedArticlesToCluster[x][:k]:\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            if len(set(newsId2UserId[newsid])&set(idsCluster[x])) == len(newsId2UserId[newsid]):\n",
    "                count += 10\n",
    "                total+=10\n",
    "                break\n",
    "    precisionVal = total/(chosenNumberOfCluster*10)\n",
    "    print(precisionVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n",
      "0.03333333333333333\n"
     ]
    }
   ],
   "source": [
    "sAt10OneUser()\n",
    "sAt10AllUsers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRR Version 1 where we compare if one user who posted the aricle exists in the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrOneUser():\n",
    "    mrr=0\n",
    "    totalmrr = 0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        mrr = 0\n",
    "        for index, y in enumerate(ascendingRankedArticlesToCluster[x]):\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            for user in newsId2UserId[newsid]:\n",
    "                if user in idsCluster[x]:\n",
    "                    mrr= (1/(index + 1))\n",
    "                    totalmrr += mrr\n",
    "                    break\n",
    "            if(mrr != 0):\n",
    "                break\n",
    "    print(totalmrr/chosenNumberOfCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRR Version 2 where we compare if all users who posted the aricle exists in the community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrAllUsers():\n",
    "    mrr=0\n",
    "    totalmrr = 0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        mrr = 0\n",
    "        for index, y in enumerate(ascendingRankedArticlesToCluster[x]):\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            if len(set(newsId2UserId[newsid])&set(idsCluster[x])) == len(newsId2UserId[newsid]):\n",
    "                mrr= (1/(index + 1))\n",
    "                totalmrr += mrr\n",
    "                break\n",
    "    print(totalmrr/chosenNumberOfCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18742772866109633\n",
      "0.04276173585261982\n"
     ]
    }
   ],
   "source": [
    "mrrOneUser()\n",
    "mrrAllUsers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewsIdsKeys = list(newsId2UserId.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionAndRecall():\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    for index, val in enumerate(NewsIdsKeys):\n",
    "        fp = 0\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        c = ascendingRankClustersToArticle[index][0][0]\n",
    "        tp = len(set(newsId2UserId[val])&set(idsCluster[c]))\n",
    "        fp = (len(idsCluster[c]) - tp)\n",
    "        fn = (len(newsId2UserId[val]) - tp)\n",
    "\n",
    "        if (tp+fp)!=0:\n",
    "            precision = precision + tp/(tp+fp)\n",
    "        if (tp+fn)!=0:\n",
    "            recall = recall + tp/(tp+fn)\n",
    "    overallPrecision = precision/len(NewsIdsKeys)\n",
    "    overallRecall = recall/len(NewsIdsKeys)\n",
    "    return (overallPrecision, overallRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.6714102128504575e-05, 0.025113231739025306)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisionAndRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.332101314784211e-05\n"
     ]
    }
   ],
   "source": [
    "x=precisionAndRecall()\n",
    "fmeasure= 2*((x[0]*x[1])/(x[0]+x[1]))\n",
    "print(fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: STORE FINAL RESULTS FOR DIFF VALUES IN CSV AND SHOW TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
