{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Capstone EB04**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following version of the code is for finding like minded user communities using regular lda analysis in order to find topics combined with clustering. This is the first of two different methods that were attempted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ragulan550/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ragulan550/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import models\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import json\n",
    "import warnings\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.corpus import words\n",
    "eng_words = words.words('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed User Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>ConceptText</th>\n",
       "      <th>userid</th>\n",
       "      <th>creationtimestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11123785801404416</td>\n",
       "      <td>tweet RT</td>\n",
       "      <td>Twitter RT (TV network)</td>\n",
       "      <td>142685766</td>\n",
       "      <td>2010-12-04 18:24:51 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16801364319404032</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>White American</td>\n",
       "      <td>81450435</td>\n",
       "      <td>2010-12-20 10:25:32 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8977557407928321</td>\n",
       "      <td>smiley face</td>\n",
       "      <td>Smiley</td>\n",
       "      <td>89099440</td>\n",
       "      <td>2010-11-28 20:16:31 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10017331400941568</td>\n",
       "      <td>kami sama rin</td>\n",
       "      <td>Kami Japanese honorifics Japanese yen</td>\n",
       "      <td>142962699</td>\n",
       "      <td>2010-12-01 17:08:12 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18759561343143936</td>\n",
       "      <td>king William the Conqueror England crowned</td>\n",
       "      <td>Charles I of England William the Conqueror Kin...</td>\n",
       "      <td>22619937</td>\n",
       "      <td>2010-12-25 20:06:42 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                   TweetText  \\\n",
       "0  11123785801404416                                    tweet RT   \n",
       "1  16801364319404032                                       WHITE   \n",
       "2   8977557407928321                                 smiley face   \n",
       "3  10017331400941568                               kami sama rin   \n",
       "4  18759561343143936  king William the Conqueror England crowned   \n",
       "\n",
       "                                         ConceptText     userid  \\\n",
       "0                            Twitter RT (TV network)  142685766   \n",
       "1                                     White American   81450435   \n",
       "2                                             Smiley   89099440   \n",
       "3              Kami Japanese honorifics Japanese yen  142962699   \n",
       "4  Charles I of England William the Conqueror Kin...   22619937   \n",
       "\n",
       "         creationtimestamp  \n",
       "0  2010-12-04 18:24:51 UTC  \n",
       "1  2010-12-20 10:25:32 UTC  \n",
       "2  2010-11-28 20:16:31 UTC  \n",
       "3  2010-12-01 17:08:12 UTC  \n",
       "4  2010-12-25 20:06:42 UTC  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loads csv from stored location\n",
    "df = pd.read_csv('../csvfiles/tweetsOnUserOnConcepts.csv', lineterminator='\\n', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all concept text for each tweet and stores in list\n",
    "tweetConcept = df.ConceptText.values.tolist()\n",
    "userIds = df.userid.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each users stores a list of their tweets stored word by word in a dictionary\n",
    "dictConcept = {}\n",
    "for i in range(len(userIds)):\n",
    "    if userIds[i] not in dictConcept:\n",
    "        dictConcept[userIds[i]] = []\n",
    "    for word in str(tweetConcept[i]).split(\" \"):\n",
    "        dictConcept[userIds[i]].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all tweets for a user\n",
    "data_final = list(dictConcept.values())\n",
    "\n",
    "#setting up corpus for lda\n",
    "id2word = corpora.Dictionary(data_final)\n",
    "texts = data_final\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA\n",
    "Multicore allows for multiple cores to be working on LDA simultaneously\n",
    "- Check Number of workers\n",
    "- Check Number of topics set<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16,\n",
      "  '0.444*\"Twitter\" + 0.053*\"TV\" + 0.052*\"series)\" + 0.050*\"This\" + '\n",
      "  '0.047*\"language\" + 0.047*\"Week\" + 0.045*\"(ABC\" + 0.037*\"ATP\" + '\n",
      "  '0.037*\"Celebrity\" + 0.014*\"of\"'),\n",
      " (2,\n",
      "  '0.219*\"Blog\" + 0.044*\"(magazine)\" + 0.038*\"(service)\" + 0.035*\"Blogger\" + '\n",
      "  '0.023*\"Don\" + 0.019*\"Tom\" + 0.016*\"Veja\" + 0.014*\"Club\" + 0.014*\"Cruise\" + '\n",
      "  '0.012*\"Electronic\"'),\n",
      " (45,\n",
      "  '0.025*\"of\" + 0.017*\"China\" + 0.016*\"United\" + 0.014*\"Bank\" + 0.014*\"States\" '\n",
      "  '+ 0.009*\"Ireland\" + 0.009*\"The\" + 0.009*\"European\" + 0.008*\"Federal\" + '\n",
      "  '0.008*\"and\"'),\n",
      " (36,\n",
      "  '0.063*\"F.C.\" + 0.023*\"football\" + 0.019*\"team\" + 0.017*\"Manchester\" + '\n",
      "  '0.015*\"national\" + 0.015*\"Association\" + 0.014*\"Cup\" + 0.013*\"FIFA\" + '\n",
      "  '0.012*\"League\" + 0.012*\"FC\"'),\n",
      " (18,\n",
      "  '0.023*\"of\" + 0.021*\"de\" + 0.013*\"SÃ£o\" + 0.012*\"Paulo\" + 0.010*\"European\" + '\n",
      "  '0.009*\"Rio\" + 0.009*\"Marcus\" + 0.009*\"Pode\" + 0.008*\"da\" + 0.008*\"do\"'),\n",
      " (46,\n",
      "  '0.029*\"BBC\" + 0.023*\"of\" + 0.020*\"The\" + 0.018*\"United\" + '\n",
      "  '0.017*\"Photography\" + 0.013*\"States\" + 0.010*\"Africa\" + 0.010*\"the\" + '\n",
      "  '0.009*\"San\" + 0.009*\"Francisco\"'),\n",
      " (32,\n",
      "  '0.027*\"California\" + 0.015*\"of\" + 0.013*\"Florida\" + 0.013*\"States\" + '\n",
      "  '0.013*\"United\" + 0.011*\"New\" + 0.010*\"San\" + 0.008*\"Angeles\" + 0.008*\"Los\" '\n",
      "  '+ 0.008*\"County,\"'),\n",
      " (15,\n",
      "  '0.043*\"Israel\" + 0.020*\"Gaza\" + 0.020*\"of\" + 0.019*\"people\" + '\n",
      "  '0.017*\"Canada\" + 0.014*\"Palestinian\" + 0.013*\"The\" + 0.012*\"United\" + '\n",
      "  '0.011*\"Egypt\" + 0.010*\"Palestine\"'),\n",
      " (23,\n",
      "  '0.016*\"of\" + 0.014*\"The\" + 0.012*\"Intelligence\" + 0.010*\"the\" + '\n",
      "  '0.010*\"Directorate\" + 0.010*\"(Israel)\" + 0.009*\"Military\" + 0.009*\"God\" + '\n",
      "  '0.008*\"Love\" + 0.008*\"(philosophy)\"'),\n",
      " (1,\n",
      "  '0.025*\"Physical\" + 0.016*\"The\" + 0.014*\"Health\" + 0.013*\"Vampire\" + '\n",
      "  '0.012*\"fitness\" + 0.012*\"Weight\" + 0.011*\"Obesity\" + 0.010*\"exercise\" + '\n",
      "  '0.009*\"Sexual\" + 0.009*\"loss\"'),\n",
      " (19,\n",
      "  '0.115*\"New\" + 0.066*\"York\" + 0.023*\"Chicago\" + 0.022*\"City\" + '\n",
      "  '0.021*\"Jersey\" + 0.011*\"of\" + 0.011*\"The\" + 0.008*\"Illinois\" + '\n",
      "  '0.008*\"Brooklyn\" + 0.007*\"Blizzard\"'),\n",
      " (41,\n",
      "  '0.035*\"Korea\" + 0.025*\"of\" + 0.024*\"United\" + 0.021*\"South\" + 0.016*\"North\" '\n",
      "  '+ 0.015*\"China\" + 0.013*\"States\" + 0.009*\"WikiLeaks\" + 0.009*\"News\" + '\n",
      "  '0.008*\"The\"'),\n",
      " (13,\n",
      "  '0.062*\"Hawaii\" + 0.023*\"of\" + 0.018*\"Aloha\" + 0.016*\"Five-0\" + 0.013*\"List\" '\n",
      "  '+ 0.012*\"sports\" + 0.012*\"Pearl\" + 0.012*\"figures\" + 0.012*\"attendance\" + '\n",
      "  '0.011*\"Harbor\"'),\n",
      " (3,\n",
      "  '0.020*\"Sudan\" + 0.019*\"Child\" + 0.016*\"violence\" + 0.015*\"Domestic\" + '\n",
      "  '0.014*\"of\" + 0.014*\"Rape\" + 0.014*\"Human\" + 0.012*\"Violence\" + '\n",
      "  '0.012*\"Virginia\" + 0.011*\"Woman\"'),\n",
      " (44,\n",
      "  '0.035*\"of\" + 0.025*\"BBC\" + 0.023*\"The\" + 0.022*\"News\" + 0.018*\"Kingdom\" + '\n",
      "  '0.015*\"London\" + 0.015*\"United\" + 0.013*\"(UK)\" + 0.011*\"the\" + 0.010*\"UK\"'),\n",
      " (37,\n",
      "  '0.040*\"Indonesia\" + 0.031*\"Jakarta\" + 0.027*\"of\" + 0.016*\"people\" + '\n",
      "  '0.015*\"Malaysia\" + 0.015*\"International\" + 0.013*\"Union\" + '\n",
      "  '0.012*\"Telecommunication\" + 0.011*\"language\" + 0.011*\"Persian\"'),\n",
      " (12,\n",
      "  '0.078*\"International\" + 0.059*\"Airport\" + 0.038*\"MCOT\" + 0.035*\"Name\" + '\n",
      "  '0.032*\"Nonproprietary\" + 0.023*\"Service\" + 0.020*\"London\" + 0.020*\"Public\" '\n",
      "  '+ 0.018*\"Thai\" + 0.016*\"Broadcasting\"'),\n",
      " (22,\n",
      "  '0.096*\"Don\\'t\" + 0.047*\"don\\'t\" + 0.047*\"tell\" + 0.047*\"ask,\" + '\n",
      "  '0.033*\"LGBT\" + 0.030*\"Gay\" + 0.025*\"of\" + 0.024*\"Act\" + 0.023*\"Repeal\" + '\n",
      "  '0.023*\"Tell\"'),\n",
      " (35,\n",
      "  '0.051*\"card\" + 0.041*\"Belarus\" + 0.031*\"Credit\" + 0.028*\"Recycling\" + '\n",
      "  '0.023*\"Visa\" + 0.022*\"Argentina\" + 0.022*\"Plastic\" + 0.020*\"Gift\" + '\n",
      "  '0.017*\"Electron\" + 0.016*\"Russia\"'),\n",
      " (7,\n",
      "  '0.037*\"Weather\" + 0.033*\"Snow\" + 0.024*\"Missouri\" + 0.023*\"Winter\" + '\n",
      "  '0.022*\"County,\" + 0.022*\"Carolina\" + 0.021*\"North\" + 0.020*\"Wind\" + '\n",
      "  '0.019*\"Rain\" + 0.017*\"Zone\"')]\n"
     ]
    }
   ],
   "source": [
    "#uncomment line below to try lda with different values\n",
    "topicNum = 47\n",
    "#lda_model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=topicNum, passes=10, workers=7)\n",
    "#lda_model.save('../LdaSaves/topics47mar10p9/lda.model_mar10_t47')\n",
    "\n",
    "#preloading an saved lda run to save time as lda takes long time to run\n",
    "\n",
    "lda_model =  models.LdaModel.load('../LDASaves/topics47mar10p9/lda.model_mar10_t47')\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Perplexity and Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -9.179285768777685\n",
      "\n",
      "Coherence Score:  0.50180613135149\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_final, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-36a28681a47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-36a28681a47a>\u001b[0m in \u001b[0;36mformat_topics_sentences\u001b[0;34m(ldamodel, corpus, texts)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_topic\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# => dominant topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mtopic_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msent_topics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_topics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_topic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_keywords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mshow_topic\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \"\"\"\n\u001b[0;32m-> 1192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopicid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_topic_terms\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \"\"\"\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopicid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# normalize to probability distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mbestn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_topics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \"\"\"\n\u001b[0;32m-> 1204\u001b[0;31m         \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_lambda\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_final):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_final)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics[0:47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating User Vectors of length K where K is number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.010639478)\n",
      "Shows a sample userVector\n",
      "[0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.5105822, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517, 0.010639517]\n"
     ]
    }
   ],
   "source": [
    "UserVectors = []\n",
    "\n",
    "#for each users shows percent contribution for that topic\n",
    "print(lda_model[corpus][1][1])\n",
    "\n",
    "for row in lda_model[corpus]:\n",
    "    temp = [0]*topicNum\n",
    "    for val in row:\n",
    "        #val is a tuple in form (topicNum, percentContributionOfTopicToUser)\n",
    "        temp[val[0]] = val[1]\n",
    "    UserVectors.append(temp)\n",
    "    \n",
    "print(\"Shows a sample userVector\")    \n",
    "print(UserVectors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Gold Standard News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>creationtimestamp</th>\n",
       "      <th>NewsId</th>\n",
       "      <th>NewsText</th>\n",
       "      <th>NewsConceptText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://on.cnn.com/9BMsbh</td>\n",
       "      <td>5813001621872640</td>\n",
       "      <td>32814009</td>\n",
       "      <td>2010-11-20 02:41:42 UTC</td>\n",
       "      <td>50637</td>\n",
       "      <td>pharmaceutical companies Big Pharma OH MY GAWD...</td>\n",
       "      <td>Pharmaceutical industry Pharmaceutical industr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://money.cnn.com/2010/12/23/pf/rich_wealth...</td>\n",
       "      <td>18123124054695937</td>\n",
       "      <td>18097177</td>\n",
       "      <td>2010-12-24 01:57:44 UTC</td>\n",
       "      <td>76310</td>\n",
       "      <td>net worth mortgages economist survey of consum...</td>\n",
       "      <td>Wealth Mortgage loan Economist Survey of Consu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.cnn.com/2010/SHOWBIZ/celebrity.news...</td>\n",
       "      <td>3517403661074433</td>\n",
       "      <td>68520890</td>\n",
       "      <td>2010-11-13 18:39:49 UTC</td>\n",
       "      <td>48949</td>\n",
       "      <td>wheelchair Toulouse-Lautrec Los Angeles, Calif...</td>\n",
       "      <td>Wheelchair Henri de Toulouse-Lautrec Los Angel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://y.ahoo.it/FzXGKS</td>\n",
       "      <td>6629109681623040</td>\n",
       "      <td>113850982</td>\n",
       "      <td>2010-11-22 08:44:37 UTC</td>\n",
       "      <td>52814</td>\n",
       "      <td>friends. You Riyadh Google Groups amd no free ...</td>\n",
       "      <td>FriendsWithYou Riyadh Google Groups Advanced M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://on.cnn.com/ic2iYo</td>\n",
       "      <td>6940757323677696</td>\n",
       "      <td>759251</td>\n",
       "      <td>2010-11-23 05:23:00 UTC</td>\n",
       "      <td>50693</td>\n",
       "      <td>thing. I Mitt Romney Massachusetts George Bush...</td>\n",
       "      <td>Treehouse of Horror VII Mitt Romney Massachuse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url            tweetid  \\\n",
       "0                           http://on.cnn.com/9BMsbh   5813001621872640   \n",
       "1  http://money.cnn.com/2010/12/23/pf/rich_wealth...  18123124054695937   \n",
       "2  http://www.cnn.com/2010/SHOWBIZ/celebrity.news...   3517403661074433   \n",
       "3                            http://y.ahoo.it/FzXGKS   6629109681623040   \n",
       "4                           http://on.cnn.com/ic2iYo   6940757323677696   \n",
       "\n",
       "      userid        creationtimestamp  NewsId  \\\n",
       "0   32814009  2010-11-20 02:41:42 UTC   50637   \n",
       "1   18097177  2010-12-24 01:57:44 UTC   76310   \n",
       "2   68520890  2010-11-13 18:39:49 UTC   48949   \n",
       "3  113850982  2010-11-22 08:44:37 UTC   52814   \n",
       "4     759251  2010-11-23 05:23:00 UTC   50693   \n",
       "\n",
       "                                            NewsText  \\\n",
       "0  pharmaceutical companies Big Pharma OH MY GAWD...   \n",
       "1  net worth mortgages economist survey of consum...   \n",
       "2  wheelchair Toulouse-Lautrec Los Angeles, Calif...   \n",
       "3  friends. You Riyadh Google Groups amd no free ...   \n",
       "4  thing. I Mitt Romney Massachusetts George Bush...   \n",
       "\n",
       "                                     NewsConceptText  \n",
       "0  Pharmaceutical industry Pharmaceutical industr...  \n",
       "1  Wealth Mortgage loan Economist Survey of Consu...  \n",
       "2  Wheelchair Henri de Toulouse-Lautrec Los Angel...  \n",
       "3  FriendsWithYou Riyadh Google Groups Advanced M...  \n",
       "4  Treehouse of Horror VII Mitt Romney Massachuse...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfGoldStandard = pd.read_csv('../csvfiles/GoldStandard.csv',  lineterminator='\\n', low_memory=False)\n",
    "dfGoldStandard.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsUserId = dfGoldStandard.userid.values.tolist()\n",
    "newsUrl = dfGoldStandard.url.values.tolist()\n",
    "newsId = dfGoldStandard.NewsId.values.tolist()\n",
    "\n",
    "#dictionary of users who posted a newsArticle\n",
    "newsId2UserId = {}\n",
    "\n",
    "for i in range(len(newsId)):\n",
    "    if newsId[i] not in newsId2UserId:\n",
    "        newsId2UserId[newsId[i]] = []\n",
    "    newsId2UserId[newsId[i]].append(newsUserId[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading another dataframe with goldstandard but only keeping unique newsids\n",
    "dfUniqueNewsId = pd.read_csv('../csvfiles/GoldStandard.csv',  lineterminator='\\n', low_memory=False)\n",
    "dfUniqueNewsId.drop_duplicates(subset='NewsId', inplace = True)\n",
    "newsArticles = dfUniqueNewsId.NewsConceptText.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing words in news articles in a list\n",
    "newsArticlesForCorpus = [x.split(' ') for x in newsArticles]\n",
    "#creating a corpus\n",
    "newsId2word = corpora.Dictionary(newsArticlesForCorpus)\n",
    "NewsArticlesCorpus = [newsId2word.doc2bow(text) for text in newsArticlesForCorpus]\n",
    "\n",
    "#using the previous lda_model with the news corpus created to get a percent contribution for each topic for each news article\n",
    "TopicDistributionOnNewsArticles = lda_model[NewsArticlesCorpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating User Vectors of length K where K is number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying sample article vector\n",
      "[0, 0, 0, 0, 0, 0.097441904, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07318355, 0, 0, 0, 0, 0, 0.16721845, 0, 0.17417727, 0, 0, 0, 0.17033984, 0, 0, 0, 0, 0, 0, 0.12391879, 0, 0, 0, 0, 0.1010313, 0, 0, 0.059913788, 0]\n"
     ]
    }
   ],
   "source": [
    "ArticleVector = []\n",
    "\n",
    "for row in TopicDistributionOnNewsArticles:\n",
    "    temp = [0]*topicNum\n",
    "    for val in row:\n",
    "        #val is a tuple in form (topicNum, percentContributionOfTopicToUser)\n",
    "        temp[val[0]] = val[1]\n",
    "    ArticleVector.append(temp)\n",
    "    \n",
    "print(\"Displaying sample article vector\")\n",
    "print(ArticleVector[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing and preloading Kmeans results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different cluster sizes to try out analysis for\n",
    "numClusters=[5, 10, 15, 20, 25, 30]\n",
    "today = datetime.datetime.now()\n",
    "\n",
    "#saving kmeans results for the differnt cluster sizes\n",
    "for x in range(len(numClusters)):\n",
    "    userVectorsFit = np.array(UserVectors)\n",
    "    #performing kmeans on the userVector to cluster users into communities\n",
    "    kmeans = KMeans(n_clusters=numClusters[x], random_state=0).fit(userVectorsFit)\n",
    "    \n",
    "    kMeansfilename = 'kMeans'+ today.strftime(\"%M%d\") + 'CSize' + str(numClusters[x])\n",
    "    pickle.dump(kmeans, open(\"../kmeansFiles/\" + kMeansfilename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen File: 'kMeans0604CSize30'\n"
     ]
    }
   ],
   "source": [
    "#change this number to a number from the [5, 10, 15, 20, 25, 30] to preload a different file\n",
    "chosenNumberOfCluster = 30\n",
    "\n",
    "#loading existing kmeans model\n",
    "kMeansfilename = 'kMeans' + today.strftime(\"%M%d\") + 'CSize' + str(chosenNumberOfCluster)\n",
    "print('Chosen File: \\''+kMeansfilename+'\\'')\n",
    "\n",
    "loadedKmeansModel = pickle.load(open(\"../kmeansFiles/\" + kMeansfilename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Users in each Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2602, 2461, 2731, 1926, 4811, 2063, 3690, 4688, 3347, 4461, 3819, 3963, 19409, 2272, 6060, 2086, 6527, 2873, 3048, 1810, 1921, 4678, 1703, 4297, 1405, 1358, 2245, 2710, 2191, 5063]\n"
     ]
    }
   ],
   "source": [
    "#creating a list to show how many users are in each cluster\n",
    "userClusters = [0]*chosenNumberOfCluster\n",
    "for i in loadedKmeansModel.labels_:\n",
    "    userClusters[i] += 1\n",
    "\n",
    "print(userClusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Indexes in each cluster, organized as an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserIndexInCluster=[]\n",
    "idsDict = list(dictConcept.keys())\n",
    "\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    UserIndexInCluster.append([])\n",
    "    \n",
    "for index, val in enumerate(loadedKmeansModel.labels_):\n",
    "    UserIndexInCluster[val].append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User ***IDs*** in each cluster, organized as an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "idsCluster = []\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    idsCluster.append([])\n",
    "    \n",
    "for index, val in enumerate(loadedKmeansModel.labels_):\n",
    "    idsCluster[val].append(idsDict[index])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Topic Distribution Per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicDistributionPerCluster=[]\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    topicDistributionPerCluster.append([])\n",
    "    \n",
    "for i,cluster in enumerate(UserIndexInCluster):\n",
    "    for userIndex in cluster:\n",
    "        topicDistributionPerCluster[i].append(UserVectors[userIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Average Topic distribution per Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00682627 0.00611497 0.00515724 0.00504671 0.01006376 0.02487488\n",
      " 0.00399558 0.00511773 0.00365652 0.01216871 0.00479767 0.0043347\n",
      " 0.00329815 0.00427418 0.00690943 0.00282821 0.01156971 0.02049791\n",
      " 0.00293984 0.00710016 0.00380569 0.00555181 0.00374264 0.00732811\n",
      " 0.00915575 0.00436225 0.0048657  0.00204496 0.00415432 0.00260818\n",
      " 0.01303746 0.582369   0.00513088 0.0033796  0.00371658 0.00315607\n",
      " 0.00293613 0.00284987 0.00461186 0.00313167 0.02004369 0.00193077\n",
      " 0.00571504 0.00293978 0.00741164 0.00316571 0.00492713]\n"
     ]
    }
   ],
   "source": [
    "averageDistributionPerCluster = []\n",
    "for x in topicDistributionPerCluster:\n",
    "    y = np.array(x)\n",
    "    listOfAverageValues = np.mean(y,axis=0)\n",
    "    averageDistributionPerCluster.append(listOfAverageValues)\n",
    "print(listOfAverageValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Articles to a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "rankArticlesToCluster=[]\n",
    "for x in range(chosenNumberOfCluster):\n",
    "    rankArticlesToCluster.append([])\n",
    "    \n",
    "for x in range (len(ArticleVector)):\n",
    "    for index,value in enumerate(averageDistributionPerCluster):\n",
    "        #finds cosine similarity between artlice vector and average vector of the cluster\n",
    "        rankArticlesToCluster[index].append(tuple((x,1 - spatial.distance.cosine(ArticleVector[x], value))))\n",
    "        \n",
    "#sorting the ranked list\n",
    "import operator\n",
    "sortedRankArticlesToCluster=[]\n",
    "for x in rankArticlesToCluster:\n",
    "    sortedRankArticlesToCluster.append(sorted(x,key=lambda x: x[1]))\n",
    "\n",
    "ascendingRankedArticlesToCluster = []\n",
    "for x in sortedRankArticlesToCluster:\n",
    "    ascendingRankedArticlesToCluster.append(list(reversed(x)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Clusters to an Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankClustersToArticle = []\n",
    "for x in range(len(ArticleVector)):\n",
    "    rankClustersToArticle.append([])\n",
    "    \n",
    "for x in range(chosenNumberOfCluster):\n",
    "    for index, value in enumerate(ArticleVector):\n",
    "        rankClustersToArticle[index].append(tuple((x, 1-spatial.distance.cosine(value, averageDistributionPerCluster[x]))))\n",
    "        \n",
    "#sorting the ranked list\n",
    "\n",
    "sortedRankClustersToArticle=[]\n",
    "for x in rankClustersToArticle:\n",
    "    sortedRankClustersToArticle.append(sorted(x,key=lambda x: x[1]))\n",
    "\n",
    "ascendingRankClustersToArticle = []\n",
    "for x in sortedRankClustersToArticle:\n",
    "    ascendingRankClustersToArticle.append(list(reversed(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S@10 Version 1 where we compare if one user who posted the aricle exists in the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sAt10OneUser():\n",
    "    k=10\n",
    "    total=0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        count = 0;\n",
    "        for y in ascendingRankedArticlesToCluster[x][:k]:\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            for user in newsId2UserId[newsid]:\n",
    "                if user in idsCluster[x]:\n",
    "                    count+=10\n",
    "                    total+=10\n",
    "                    break\n",
    "            if count != 0:\n",
    "                break\n",
    "    precisionVal = total/(chosenNumberOfCluster*10)\n",
    "    print(precisionVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S@10 Version 2 where we compare if all users who posted the aricle exists in the community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sAt10AllUsers():\n",
    "    k=10\n",
    "    total=0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        count = 0;\n",
    "        for y in ascendingRankedArticlesToCluster[x][:k]:\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            if len(set(newsId2UserId[newsid])&set(idsCluster[x])) == len(newsId2UserId[newsid]):\n",
    "                count += 10\n",
    "                total+=10\n",
    "                break\n",
    "    precisionVal = total/(chosenNumberOfCluster*10)\n",
    "    print(precisionVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36666666666666664\n",
      "0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "sAt10OneUser()\n",
    "sAt10AllUsers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRR Version 1 where we compare if one user who posted the aricle exists in the community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrOneUser():\n",
    "    mrr=0\n",
    "    totalmrr = 0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        mrr = 0\n",
    "        for index, y in enumerate(ascendingRankedArticlesToCluster[x]):\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            for user in newsId2UserId[newsid]:\n",
    "                if user in idsCluster[x]:\n",
    "                    mrr= (1/(index + 1))\n",
    "                    totalmrr += mrr\n",
    "                    break\n",
    "            if(mrr != 0):\n",
    "                break\n",
    "    print(totalmrr/chosenNumberOfCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRR Version 2 where we compare if all users who posted the aricle exists in the community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrrAllUsers():\n",
    "    mrr=0\n",
    "    totalmrr = 0\n",
    "    for x in range(len(ascendingRankedArticlesToCluster)):\n",
    "        mrr = 0\n",
    "        for index, y in enumerate(ascendingRankedArticlesToCluster[x]):\n",
    "            newsid = int(dfUniqueNewsId.iloc[[y[0]]].NewsId)\n",
    "            if len(set(newsId2UserId[newsid])&set(idsCluster[x])) == len(newsId2UserId[newsid]):\n",
    "                mrr= (1/(index + 1))\n",
    "                totalmrr += mrr\n",
    "                break\n",
    "    print(totalmrr/chosenNumberOfCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21833547028700867\n",
      "0.029922369836447076\n"
     ]
    }
   ],
   "source": [
    "mrrOneUser()\n",
    "mrrAllUsers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewsIdsKeys = list(newsId2UserId.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precisionAndRecall():\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    \n",
    "    for index, val in enumerate(NewsIdsKeys):\n",
    "        fp = 0\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        c = ascendingRankClustersToArticle[index][0][0]\n",
    "        tp = len(set(newsId2UserId[val])&set(idsCluster[c]))\n",
    "        fp = (len(idsCluster[c]) - tp)\n",
    "        fn = (len(newsId2UserId[val]) - tp)\n",
    "\n",
    "        if (tp+fp)!=0:\n",
    "            precision = precision + tp/(tp+fp)\n",
    "        if (tp+fn)!=0:\n",
    "            recall = recall + tp/(tp+fn)\n",
    "    overallPrecision = precision/len(NewsIdsKeys)\n",
    "    overallRecall = recall/len(NewsIdsKeys)\n",
    "    return (overallPrecision, overallRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.960150378862192e-05, 0.06152784406574581)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisionAndRecall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00011908764837632181\n"
     ]
    }
   ],
   "source": [
    "x=precisionAndRecall()\n",
    "fmeasure= 2*((x[0]*x[1])/(x[0]+x[1]))\n",
    "print(fmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: STORE FINAL RESULTS FOR DIFF VALUES IN CSV AND SHOW TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
